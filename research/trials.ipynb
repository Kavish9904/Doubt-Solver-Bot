{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"e353333b-b041-4e67-888c-ea37f8b89dc5\")\n",
    "\n",
    "# Test the connection\n",
    "print(pc.list_indexes().names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PINECONE_API_KEY = \"e353333b-b041-4e67-888c-ea37f8b89dc5\"\n",
    "#PINECONE_API_ENV = \"us-east-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    print(\"Function load_pdf is running\")\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf(\"D:\\End-to-end-Chatbot-using-Llama2\\data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print(\"length of my chunk:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pinecone index creation and data insertion\n",
    "index_name = \"chatbot\"\n",
    "\n",
    "# Check if index exists, create if it doesn't\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # dimension of all-MiniLM-L6-v2 embeddings\n",
    "        metric=\"cosine\"\n",
    "    )\n",
    "\n",
    "# Get the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Debug: Check if text_chunks exists and is not empty\n",
    "if 'text_chunks' not in globals():\n",
    "    print(\"Error: text_chunks is not defined. Make sure you've run the previous cells.\")\n",
    "elif not text_chunks:\n",
    "    print(\"Error: text_chunks is empty. Check if the PDF loading was successful.\")\n",
    "else:\n",
    "    print(f\"Number of text chunks: {len(text_chunks)}\")\n",
    "\n",
    "    # Create embeddings for text chunks\n",
    "    texts = [t.page_content for t in text_chunks]\n",
    "    print(f\"Number of texts: {len(texts)}\")\n",
    "\n",
    "    embeddings_list = embeddings.embed_documents(texts)\n",
    "    print(f\"Number of embeddings: {len(embeddings_list)}\")\n",
    "\n",
    "    # Prepare data for upsert\n",
    "    vectors = [\n",
    "        (str(i), embedding, {\"text\": text})\n",
    "        for i, (embedding, text) in enumerate(zip(embeddings_list, texts))\n",
    "    ]\n",
    "    print(f\"Number of vectors: {len(vectors)}\")\n",
    "\n",
    "    # Batch upsert to Pinecone\n",
    "    batch_size = 100  # You can adjust this value\n",
    "    for i in tqdm(range(0, len(vectors), batch_size)):\n",
    "        batch = vectors[i:i+batch_size]\n",
    "        index.upsert(vectors=batch)\n",
    "\n",
    "    print(\"Data has been successfully loaded into Pinecone index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pinecone-client langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "from pinecone import Pinecone\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key=\"e353333b-b041-4e67-888c-ea37f8b89dc5\")\n",
    "\n",
    "# Get the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Initialize HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create a custom embedding function\n",
    "def embed_query(query):\n",
    "    return embeddings.embed_query(query)\n",
    "\n",
    "# Create a Pinecone vector store for use with LangChain\n",
    "docsearch = LangchainPinecone(\n",
    "    index,\n",
    "    embed_query,\n",
    "    \"text\"  # This should match the metadata field name you used when uploading vectors\n",
    ")\n",
    "\n",
    "# Create a custom embedding function\n",
    "def embed_query(query):\n",
    "    return embeddings.embed_query(query)\n",
    "\n",
    "query = \"What is Literacy\"\n",
    "\n",
    "# Modify the similarity_search method to use keyword arguments\n",
    "def modified_similarity_search(query, k=3, **kwargs):\n",
    "    query_embedding = embed_query(query)\n",
    "    results = index.query(vector=query_embedding, top_k=k, include_metadata=True, namespace='chatbot')\n",
    "    \n",
    "    documents = []\n",
    "    for result in results['matches']:\n",
    "        documents.append(result['metadata']['text'])\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Use the modified similarity search\n",
    "docs = modified_similarity_search(query, k=3)\n",
    "\n",
    "print(\"Result:\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs={\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=CTransformers(model=\"D:\\End-to-end-Chatbot-using-Llama2\\model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':512,\n",
    "                          'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(f\"Input Prompt (type 'quit' to exit): \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Exiting the program...\")\n",
    "        break\n",
    "    \n",
    "    result = qa.invoke({\"query\": user_input})\n",
    "    print(\"Response : \", result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
